{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb#scrollTo=8Ej3snbu3edU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the dataset\n",
    "# !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "# # # unzip it\n",
    "# !tar -zxf ./data/aclImdb_v1/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "# imdb[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.mps.is_available()) #the MacOS is higher than 12.3+\n",
    "print(torch.backends.mps.is_built()) #MPS is activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "example_text = \"I will watch Memento tonight, I heard it's a good movie.\"\n",
    "gpt2_input = tokenizer(example_text, padding=\"max_length\", max_length=20, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input:\")\n",
    "print(gpt2_input['input_ids'])\n",
    "print()\n",
    "\n",
    "print(\"Attention mask:\")\n",
    "print(gpt2_input[\"attention_mask\"])\n",
    "print()\n",
    "\n",
    "print(\"Decoded:\")\n",
    "example_text = tokenizer.decode(gpt2_input.input_ids[0])\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsDataset(Dataset):\n",
    "  r\"\"\"PyTorch Dataset class for loading data.\n",
    "\n",
    "  This is where the data parsing happens.\n",
    "\n",
    "  This class is built with reusability in mind: it can be used as is as.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    path (:obj:`str`):\n",
    "        Path to the data partition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, path, use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "    if not os.path.isdir(path):\n",
    "      # Raise error if path is invalid.\n",
    "      raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "\n",
    "    self.texts = []\n",
    "    self.labels = []\n",
    "    # Since the labels are defined by folders with data we loop \n",
    "    # through each label.\n",
    "    for label in ['pos', 'neg']:\n",
    "      sentiment_path = os.path.join(path, label)\n",
    "\n",
    "      # Get all files from path.\n",
    "      files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "      # Go through each file and read its content.\n",
    "      for i, file_name in enumerate(tqdm(files_names, desc=f'{label} files')):\n",
    "        file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "        # Read content.\n",
    "        content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "        # # Fix any unicode issues.\n",
    "        # content = fix_text(content)\n",
    "        # Save content.\n",
    "        self.texts.append(content)\n",
    "        # Save encode labels.\n",
    "        self.labels.append(label)\n",
    "\n",
    "        # if i > 100:\n",
    "        #   break\n",
    "\n",
    "    # Number of exmaples.\n",
    "    self.n_examples = len(self.labels)\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    \"When used `len` return the number of examples.\"\n",
    "    return self.n_examples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    r\"\"\"Given an index return an example from the position.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "      item (:obj:`int`):\n",
    "          Index position to pick an example to return.\n",
    "\n",
    "    Returns:\n",
    "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "      asociated labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "train_dataset = MovieReviewsDataset(path='./data/aclImdb_v1/aclImdb/train', use_tokenizer=tokenizer)\n",
    "train_dataset.__len__()\n",
    "\n",
    "# train_dataset.__getitem__(0)\n",
    "# train_dataset.texts[0]\n",
    "# train_dataset.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "  r\"\"\"\n",
    "  Train pytorch model on a single pass through the data loader.\n",
    "\n",
    "  It will use the global variable `model` which is the transformer model \n",
    "  loaded on `_device` that we want to train on.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "      optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
    "          Optimizer used for training.\n",
    "\n",
    "      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "          PyTorch scheduler.\n",
    "\n",
    "      device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "\n",
    "      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss].\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables.\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  # Total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model into training mode.\n",
    "  model.train()\n",
    "\n",
    "  # For each batch of training data...\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # Add original labels - use later for evaluation.\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "    \n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "    \n",
    "    # Always clear any previously calculated gradients before performing a\n",
    "    # backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Perform a forward pass (evaluate the model on this training batch).\n",
    "    # This will return the loss (rather than the model output) because we\n",
    "    # have provided the `labels`.\n",
    "    # The documentation for this a bert model function is here: \n",
    "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # The call to `model` always returns a tuple, so we need to pull the \n",
    "    # loss value out of the tuple along with the logits. We will use logits\n",
    "    # later to calculate training accuracy.\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    # Accumulate the training loss over all of the batches so that we can\n",
    "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "    # single value; the `.item()` function just returns the Python value \n",
    "    # from the tensor.\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Perform a backward pass to calculate the gradients.\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0.\n",
    "    # This is to help prevent the \"exploding gradients\" problem.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient.\n",
    "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "    # modified based on their gradients, the learning rate, etc.\n",
    "    optimizer_.step()\n",
    "\n",
    "    # Update the learning rate.\n",
    "    scheduler_.step()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Convert these logits to list of predicted labels values.\n",
    "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "  \n",
    "  # Return all true labels and prediction for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, device_):\n",
    "  r\"\"\"Validation function to evaluate model performance on a \n",
    "  separate set of data.\n",
    "\n",
    "  This function will return the true and predicted labels so we can use later\n",
    "  to evaluate the model's performance.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "    device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "    \n",
    "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss]\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  #total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model in evaluation mode--the dropout layers behave differently\n",
    "  # during evaluation.\n",
    "  model.eval()\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # add original labels\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up validation\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # get predicitons to list\n",
    "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "        # update list\n",
    "        predictions_labels += predict_content\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "  # Return all true labels and prediciton for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuration...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "# print(model_config)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = MovieReviewsDataset(path='./data/aclImdb_v1/aclImdb/train', use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  MovieReviewsDataset(path='./data/aclImdb_v1/aclImdb/test', use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print()\n",
    "  print('Training on batches...')\n",
    "  # Perform one full pass over the training set.\n",
    "  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "  train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "  # Get prediction form model on validation data. \n",
    "  print('Validation on batches...')\n",
    "  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "  # Print loss and accuracy values to see how training evolves.\n",
    "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "  print()\n",
    "\n",
    "  # Store the loss value for plotting the learning curve.\n",
    "  all_loss['train_loss'].append(train_loss)\n",
    "  all_loss['val_loss'].append(val_loss)\n",
    "  all_acc['train_acc'].append(train_acc)\n",
    "  all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Magnify intervals where font size matters\n",
    "MAGNIFY_INTERVALS = [0.1, 1]\n",
    "\n",
    "# Min and max appropriate font sizes\n",
    "FONT_RANGE = [10.5, 50]\n",
    "\n",
    "# Maximum allowed magnify. This will get multiplied by 0 - 1 value.\n",
    "MAX_MAGNIFY = 15\n",
    "\n",
    "# Increase font for title ratio.\n",
    "TITLE_FONT_RATIO = 1.8\n",
    "\n",
    "def plot_dict(dict_arrays, start_step=0, step_size=1, use_title=None, points_values=False, points_round=3,\n",
    "              use_xlabel=None, use_xticks=True, use_rotation_xticks=0, xticks_labels=None, use_ylabel=None,\n",
    "              style_sheet='ggplot', use_grid=True, use_linestyles=None, font_size=None, width=3, height=1, magnify=1.2,\n",
    "              use_dpi=50, path=None, show_plot=True):\n",
    "    r\"\"\"\n",
    "    Create plot from a single array of values.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        dict_arrays (:obj:`dict([list])`):\n",
    "            Dictionary of arrays that will get plotted. The keys in dictionary are used as labels and the values as\n",
    "            arrays that get plotted.\n",
    "\n",
    "        start_step (:obj:`int`, `optional`, defaults to :obj:`0`):\n",
    "            Starting value of plot.This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        step_size (:obj:`int`, `optional`, defaults to :obj:`q`):\n",
    "            Steps shows on x-axis. Change if each steps is different than 1.This argument is optional and it has a\n",
    "            default value attributed inside the function.\n",
    "\n",
    "        use_title (:obj:`int`, `optional`):\n",
    "            Title on top of plot. This argument is optional and it will have a `None` value attributed\n",
    "            inside the function.\n",
    "\n",
    "        points_values (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Display each point value on the plot. This argument is optional and it has a default value attributed\n",
    "            inside the function.\n",
    "\n",
    "        points_round (:obj:`int`, `optional`, defaults to :obj:`1`):\n",
    "            Round decimal valus for points values. This argument is optional and it has a default value attributed\n",
    "            inside the function.\n",
    "\n",
    "        use_xlabel (:obj:`str`, `optional`):\n",
    "            Label to use for x-axis value meaning. This argument is optional and it will have a `None` value attributed\n",
    "            inside the function.\n",
    "\n",
    "        use_xticks (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Display x-axis tick values (the values at each point). This argument is optional and it has a default\n",
    "            value attributed inside the function.\n",
    "\n",
    "        use_ylabel (:obj:`str`, `optional`):\n",
    "            Label to use for y-axis value meaning. This argument is optional and it will have a `None` value attributed\n",
    "            inside the function.\n",
    "\n",
    "        style_sheet (:obj:`str`, `optional`, defaults to :obj:`ggplot`):\n",
    "            Style of plot. Use plt.style.available to show all styles. This argument is optional and it has a default\n",
    "            value attributed inside the function.\n",
    "\n",
    "        use_grid (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Show grid on plot or not. This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        use_linestyles (:obj:`str`, `optional`, defaults to :obj:`-`):\n",
    "            Style to use on line from ['-', '--', '-.', ':']. This argument is optional and it has a default\n",
    "            value attributed inside the function.\n",
    "\n",
    "        font_size (:obj:`int` or `float`, `optional`):\n",
    "            Font size to use across the plot. By default this function will adjust font size depending on `magnify`\n",
    "            value. If this value is set, it will ignore the `magnify` recommended font size. The title font size is by\n",
    "            default `1.8` greater than font-size. This argument is optional and it will have a `None` value attributed\n",
    "            inside the function.\n",
    "\n",
    "        width (:obj:`int`, `optional`, defaults to :obj:`3`):\n",
    "            Horizontal length of plot. This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        height (:obj:`int`, `optional`, defaults to :obj:`1`):\n",
    "            Height length of plot in inches. This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        magnify (:obj:`float`, `optional`, defaults to :obj:`0.1`):\n",
    "            Ratio increase of both with and height keeping the same ratio size. This argument is optional and it has a\n",
    "            default value attributed inside the function.\n",
    "\n",
    "        use_dpi (:obj:`int`, `optional`, defaults to :obj:`50`):\n",
    "            Print resolution is measured in dots per inch (or “DPI”). This argument is optional and it has a default\n",
    "            value attributed inside the function.\n",
    "\n",
    "        path (:obj:`str`, `optional`):\n",
    "            Path and file name of plot saved as image. If want to save in current path just pass in the file name.\n",
    "            This argument is optional and it will have a None value attributed inside the function.\n",
    "\n",
    "        show_plot (:obj:`bool`, `optional`, defaults to :obj:`1`):\n",
    "            if you want to call `plt.show()`. or not (if you run on a headless server). This argument is optional and\n",
    "            it has a default value attributed inside the function.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "        ValueError: If `dict_arrays` is not of type `dictionary`.\n",
    "\n",
    "        ValueError: If `dict_arrays` doesn't have string keys.\n",
    "\n",
    "        ValueError: If `dict_arrays` doesn't have array values.\n",
    "\n",
    "        ValueError: If `style_sheet` is not valid.\n",
    "\n",
    "        ValueError: If `use_linestyle` is not valid.\n",
    "\n",
    "        ValueError: If `points_values`of type list don't have same length as `dict_arrays`.\n",
    "\n",
    "        DeprecationWarning: If `magnify` is se to values that don't belong to [0, 1] values.\n",
    "\n",
    "        ValueError: If `font_size` is not `None` and smaller or equal to 0.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if `dict_arrays` is the correct format.\n",
    "    if not isinstance(dict_arrays, dict):\n",
    "        # Raise value error.\n",
    "        raise ValueError(\"`dict_arrays` needs to be a dictionary of values!\")\n",
    "\n",
    "    # Check each label\n",
    "    for label, array in dict_arrays.items():\n",
    "        # Check if format is correct.\n",
    "        if not isinstance(label, str):\n",
    "            # Raise value error.\n",
    "            raise ValueError(\"`dict_arrays` needs string keys!\")\n",
    "        if not isinstance(array, list) or isinstance(array, np.ndarray):\n",
    "            # Raise value error.\n",
    "            raise ValueError(\"`dict_arrays` needs lists values!\")\n",
    "\n",
    "    # Make sure style sheet is correct.\n",
    "    if style_sheet in plt.style.available:\n",
    "        # Set style of plot\n",
    "        plt.style.use(style_sheet)\n",
    "    else:\n",
    "        # Style is not correct.\n",
    "        raise ValueError(\"`style_sheet=%s` is not in the supported styles: %s\" % (str(style_sheet),\n",
    "                                                                                  str(plt.style.available)))\n",
    "\n",
    "    # Make sure `magnify` is in right range.\n",
    "    if magnify > 1 or magnify <= 0:\n",
    "        # Deprecation warning from last time.\n",
    "        warnings.warn(f'`magnify` needs to have value in [0,1]! `{magnify}` will be converted to `0.1` as default.',\n",
    "                      DeprecationWarning)\n",
    "        # Convert to regular value 0.1.\n",
    "        magnify = 0.1\n",
    "\n",
    "    # all linestyles.\n",
    "    linestyles = ['-', '--', '-.', ':']\n",
    "\n",
    "    # Make sure `font_size` is set right.\n",
    "    if (font_size is not None) and (font_size <= 0):\n",
    "        # Raise value error -  is not correct.\n",
    "        raise ValueError(f'`font_size` needs to be positive number! Invalid value {font_size}')\n",
    "\n",
    "    # Font size select custom or adjusted on `magnify` value.\n",
    "    font_size = font_size if font_size is not None else np.interp(magnify, MAGNIFY_INTERVALS, FONT_RANGE)\n",
    "\n",
    "    # Font variables dictionary. Keep it in this format for future updates.\n",
    "    font_dict = dict(\n",
    "        family='DejaVu Sans',\n",
    "        color='black',\n",
    "        weight='normal',\n",
    "        size=font_size,\n",
    "    )\n",
    "\n",
    "    # If single style value is passed, use it on all arrays.\n",
    "    if use_linestyles is None:\n",
    "        use_linestyles = ['-'] * len(dict_arrays)\n",
    "\n",
    "    else:\n",
    "        # Check if linestyle is set right.\n",
    "        for use_linestyle in use_linestyles:\n",
    "            if use_linestyle not in linestyles:\n",
    "                # Raise error.\n",
    "                raise ValueError(\"`linestyle=%s` is not in the styles: %s!\" % (str(use_linestyle), str(linestyles)))\n",
    "\n",
    "    # Check `points_value` type - it can be bool or list(bool).\n",
    "    if isinstance(points_values, bool):\n",
    "        # Convert to list.\n",
    "        points_values = [points_values] * len(dict_arrays)\n",
    "    elif isinstance(points_values, list) and (len(points_values) != len(dict_arrays)):\n",
    "        raise ValueError('`points_values` of type `list` must have same length as dictionary!')\n",
    "\n",
    "    # Single plot figure.\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    # Use maximum length of steps. In case each arrya has different lengths.\n",
    "    max_steps = []\n",
    "\n",
    "    # Plot each array.\n",
    "    for index, (use_label, array) in enumerate(dict_arrays.items()):\n",
    "        # Set steps plotted on x-axis - we can use step if 1 unit has different value.\n",
    "        if start_step > 0:\n",
    "            # Offset all steps by start_step.\n",
    "            steps = np.array(range(0, len(array))) * step_size + start_step\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "        else:\n",
    "            steps = np.array(range(1, len(array) + 1)) * step_size\n",
    "            max_steps = steps if len(max_steps) < len(steps) else max_steps\n",
    "\n",
    "        # Plot array as a single line.\n",
    "        plt.plot(steps, array, linestyle=use_linestyles[index], label=use_label)\n",
    "\n",
    "        # Plots points values.\n",
    "        if points_values[index]:\n",
    "            # Loop through each point and plot the label.\n",
    "            for x, y in zip(steps, array):\n",
    "                # Add text label to plot.\n",
    "                plt.text(x, y, str(round(y, points_round)), fontdict=font_dict)\n",
    "\n",
    "    # Set horizontal axis name.\n",
    "    plt.xlabel(use_xlabel, fontdict=font_dict)\n",
    "\n",
    "    # Use x ticks with steps or labels.\n",
    "    plt.xticks(max_steps, xticks_labels, rotation=use_rotation_xticks) if use_xticks else None\n",
    "\n",
    "    # Set vertical axis name.\n",
    "    plt.ylabel(use_ylabel, fontdict=font_dict)\n",
    "\n",
    "    # Adjust both axis labels font size at same time.\n",
    "    plt.tick_params(labelsize=font_dict['size'])\n",
    "\n",
    "    # Place legend best position.\n",
    "    plt.legend(loc='best', fontsize=font_dict['size'])\n",
    "\n",
    "    # Adjust font for title.\n",
    "    font_dict['size'] *= TITLE_FONT_RATIO\n",
    "\n",
    "    # Set title of figure.\n",
    "    plt.title(use_title, fontdict=font_dict)\n",
    "\n",
    "    # Rescale `magnify` to be used on inches.\n",
    "    magnify *= MAX_MAGNIFY\n",
    "\n",
    "    # Display grid depending on `use_grid`.\n",
    "    plt.grid(use_grid)\n",
    "\n",
    "    # Make figure nice.\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get figure object from plot.\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    # Get size of figure.\n",
    "    figsize = fig.get_size_inches()\n",
    "\n",
    "    # Change size depending on height and width variables.\n",
    "    figsize = [figsize[0] * width * magnify, figsize[1] * height * magnify]\n",
    "\n",
    "    # Set the new figure size with magnify.\n",
    "    fig.set_size_inches(figsize)\n",
    "\n",
    "    # There is an error when DPI and plot size are too large!\n",
    "    try:\n",
    "        # Save figure to image if path is set.\n",
    "        fig.savefig(path, dpi=use_dpi, bbox_inches='tight') if path is not None else None\n",
    "    except ValueError:\n",
    "        # Deprecation warning from last time.\n",
    "        warnings.warn(f'`magnify={magnify // 15}` is to big in combination'\n",
    "                      f' with `use_dpi={use_dpi}`! Try using lower values for'\n",
    "                      f' `magnify` and/or `use_dpi`. Image was saved in {path}'\n",
    "                      f' with `use_dpi=50 and `magnify={magnify // 15}`!', Warning)\n",
    "        # Set DPI to smaller value and warn user to use smaller magnify or smaller dpi.\n",
    "        use_dpi = 50\n",
    "        # Save figure to image if path is set.\n",
    "        fig.savefig(path, dpi=use_dpi, bbox_inches='tight') if path is not None else None\n",
    "\n",
    "    # Show plot.\n",
    "    plt.show() if show_plot is True else None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gpt2_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
